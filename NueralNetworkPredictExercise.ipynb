{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "intellectual-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mute warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "hawaiian-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "all_data = pd.read_csv(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "extensive-grounds",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Exercise_Duration_Total</th>\n",
       "      <th>Exercise_METmin_Total</th>\n",
       "      <th>Planned Day Off_binary</th>\n",
       "      <th>Not Enough Time_binary</th>\n",
       "      <th>Lacked Positive Feelings_binary</th>\n",
       "      <th>Had Negative Feelings_binary</th>\n",
       "      <th>Lacked Energy_binary</th>\n",
       "      <th>Felt Fatigued_binary</th>\n",
       "      <th>Blood sugar management issue_binary</th>\n",
       "      <th>Sick</th>\n",
       "      <th>...</th>\n",
       "      <th>NegativePost</th>\n",
       "      <th>EnergyPost</th>\n",
       "      <th>FatiguePost</th>\n",
       "      <th>Mean_total</th>\n",
       "      <th>CV_total</th>\n",
       "      <th>Time High (%)_total</th>\n",
       "      <th>Time In Range (%)_total</th>\n",
       "      <th>Time Low (%)_total</th>\n",
       "      <th>Mean_nighttime</th>\n",
       "      <th>Time Low (%)_nighttime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>114.472028</td>\n",
       "      <td>20.43765825</td>\n",
       "      <td>0</td>\n",
       "      <td>96.5034965</td>\n",
       "      <td>3.496503497</td>\n",
       "      <td>114.1354167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>132.28125</td>\n",
       "      <td>25.85976222</td>\n",
       "      <td>5.902777778</td>\n",
       "      <td>89.58333333</td>\n",
       "      <td>4.513888889</td>\n",
       "      <td>107.6354167</td>\n",
       "      <td>13.54166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.96875</td>\n",
       "      <td>34.98702893</td>\n",
       "      <td>24.65277778</td>\n",
       "      <td>68.40277778</td>\n",
       "      <td>6.944444444</td>\n",
       "      <td>140.46875</td>\n",
       "      <td>11.45833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0840336</td>\n",
       "      <td>35.33584081</td>\n",
       "      <td>0</td>\n",
       "      <td>76.47058824</td>\n",
       "      <td>23.52941176</td>\n",
       "      <td>53.60869565</td>\n",
       "      <td>97.82608696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>No diaries (baseline)</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>126.13879</td>\n",
       "      <td>27.64118317</td>\n",
       "      <td>8.540925267</td>\n",
       "      <td>91.45907473</td>\n",
       "      <td>0</td>\n",
       "      <td>118.7708333</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>60</td>\n",
       "      <td>120.0</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>N/.A</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>115.9965035</td>\n",
       "      <td>21.80354914</td>\n",
       "      <td>0</td>\n",
       "      <td>99.3006993</td>\n",
       "      <td>0.699300699</td>\n",
       "      <td>121.6354167</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1961</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>120.1060606</td>\n",
       "      <td>24.93535863</td>\n",
       "      <td>0</td>\n",
       "      <td>95.45454545</td>\n",
       "      <td>4.545454545</td>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>159.5774059</td>\n",
       "      <td>25.57146553</td>\n",
       "      <td>28.87029289</td>\n",
       "      <td>71.12970711</td>\n",
       "      <td>0</td>\n",
       "      <td>146.7282609</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>130.8958333</td>\n",
       "      <td>20.96629896</td>\n",
       "      <td>5.555555556</td>\n",
       "      <td>94.44444444</td>\n",
       "      <td>0</td>\n",
       "      <td>124.84375</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>141.0522648</td>\n",
       "      <td>24.07310433</td>\n",
       "      <td>11.8466899</td>\n",
       "      <td>88.1533101</td>\n",
       "      <td>0</td>\n",
       "      <td>154.6210526</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1965 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Exercise_Duration_Total  Exercise_METmin_Total Planned Day Off_binary  \\\n",
       "0                           0                    0.0  No diaries (baseline)   \n",
       "1                           0                    0.0  No diaries (baseline)   \n",
       "2                           0                    0.0  No diaries (baseline)   \n",
       "3                           0                    0.0  No diaries (baseline)   \n",
       "4                           0                    0.0  No diaries (baseline)   \n",
       "...                       ...                    ...                    ...   \n",
       "1960                       60                  120.0                   N/.A   \n",
       "1961                        0                    0.0                      0   \n",
       "1962                        0                    0.0                      0   \n",
       "1963                        0                    0.0                      0   \n",
       "1964                        0                    0.0                      0   \n",
       "\n",
       "     Not Enough Time_binary Lacked Positive Feelings_binary  \\\n",
       "0     No diaries (baseline)           No diaries (baseline)   \n",
       "1     No diaries (baseline)           No diaries (baseline)   \n",
       "2     No diaries (baseline)           No diaries (baseline)   \n",
       "3     No diaries (baseline)           No diaries (baseline)   \n",
       "4     No diaries (baseline)           No diaries (baseline)   \n",
       "...                     ...                             ...   \n",
       "1960                   N/.A                            N/.A   \n",
       "1961                      0                               0   \n",
       "1962                      1                               0   \n",
       "1963                      1                               0   \n",
       "1964                      1                               0   \n",
       "\n",
       "     Had Negative Feelings_binary   Lacked Energy_binary  \\\n",
       "0           No diaries (baseline)  No diaries (baseline)   \n",
       "1           No diaries (baseline)  No diaries (baseline)   \n",
       "2           No diaries (baseline)  No diaries (baseline)   \n",
       "3           No diaries (baseline)  No diaries (baseline)   \n",
       "4           No diaries (baseline)  No diaries (baseline)   \n",
       "...                           ...                    ...   \n",
       "1960                         N/.A                   N/.A   \n",
       "1961                            0                      1   \n",
       "1962                            0                      0   \n",
       "1963                            0                      0   \n",
       "1964                            0                      0   \n",
       "\n",
       "       Felt Fatigued_binary Blood sugar management issue_binary  \\\n",
       "0     No diaries (baseline)               No diaries (baseline)   \n",
       "1     No diaries (baseline)               No diaries (baseline)   \n",
       "2     No diaries (baseline)               No diaries (baseline)   \n",
       "3     No diaries (baseline)               No diaries (baseline)   \n",
       "4     No diaries (baseline)               No diaries (baseline)   \n",
       "...                     ...                                 ...   \n",
       "1960                   N/.A                                N/.A   \n",
       "1961                      1                                   0   \n",
       "1962                      0                                   0   \n",
       "1963                      0                                   0   \n",
       "1964                      0                                   0   \n",
       "\n",
       "                       Sick  ... NegativePost EnergyPost FatiguePost  \\\n",
       "0     No diaries (baseline)  ...          NaN        NaN         NaN   \n",
       "1     No diaries (baseline)  ...          NaN        NaN         NaN   \n",
       "2     No diaries (baseline)  ...          NaN        NaN         NaN   \n",
       "3     No diaries (baseline)  ...          NaN        NaN         NaN   \n",
       "4     No diaries (baseline)  ...          NaN        NaN         NaN   \n",
       "...                     ...  ...          ...        ...         ...   \n",
       "1960                      0  ...          NaN        NaN         NaN   \n",
       "1961                      0  ...          NaN        NaN         NaN   \n",
       "1962                      0  ...          NaN        NaN         NaN   \n",
       "1963                      0  ...          NaN        NaN         NaN   \n",
       "1964                      0  ...          NaN        NaN         NaN   \n",
       "\n",
       "       Mean_total     CV_total Time High (%)_total Time In Range (%)_total  \\\n",
       "0      114.472028  20.43765825                   0              96.5034965   \n",
       "1       132.28125  25.85976222         5.902777778             89.58333333   \n",
       "2       141.96875  34.98702893         24.65277778             68.40277778   \n",
       "3     108.0840336  35.33584081                   0             76.47058824   \n",
       "4       126.13879  27.64118317         8.540925267             91.45907473   \n",
       "...           ...          ...                 ...                     ...   \n",
       "1960  115.9965035  21.80354914                   0              99.3006993   \n",
       "1961  120.1060606  24.93535863                   0             95.45454545   \n",
       "1962  159.5774059  25.57146553         28.87029289             71.12970711   \n",
       "1963  130.8958333  20.96629896         5.555555556             94.44444444   \n",
       "1964  141.0522648  24.07310433          11.8466899              88.1533101   \n",
       "\n",
       "     Time Low (%)_total Mean_nighttime Time Low (%)_nighttime  \n",
       "0           3.496503497    114.1354167                      0  \n",
       "1           4.513888889    107.6354167            13.54166667  \n",
       "2           6.944444444      140.46875            11.45833333  \n",
       "3           23.52941176    53.60869565            97.82608696  \n",
       "4                     0    118.7708333                      0  \n",
       "...                 ...            ...                    ...  \n",
       "1960        0.699300699    121.6354167                      0  \n",
       "1961        4.545454545              .                      .  \n",
       "1962                  0    146.7282609                      0  \n",
       "1963                  0      124.84375                      0  \n",
       "1964                  0    154.6210526                      0  \n",
       "\n",
       "[1965 rows x 30 columns]"
      ]
     },
     "execution_count": 381,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create dataframe with 30 important features \n",
    "\n",
    "labels = all_data[[\"Participant\", \"Mean_total\", \"CV_total\", \"Time High (%)_total\", \"Time In Range (%)_total\", \"Time Low (%)_total\", \n",
    "               \"Mean_nighttime\", \"Time Low (%)_nighttime\"]]\n",
    "thirty_classes = [\"Exercise_Duration_Total\", \"Exercise_METmin_Total\", \"Planned Day Off_binary\", \"Not Enough Time_binary\",\n",
    "              \"Lacked Positive Feelings_binary\", \"Had Negative Feelings_binary\", \"Lacked Energy_binary\", \n",
    "               \"Felt Fatigued_binary\", \"Blood sugar management issue_binary\", \"Sick\", \"Morning Fear of Hypoglycemia\",\n",
    "              \"Evening Fear of Hypoglycemia\", \"Sleep Quality\", \"FOHPre\", \"PositivePre\", \"NegativePre\", \n",
    "                \"EnergyPre\", \"FatiguePre\", \"FOHPost\", \"PositivePost\", \"NegativePost\", \"EnergyPost\",\t\"FatiguePost\",\n",
    "              \"Mean_total\", \"CV_total\", \"Time High (%)_total\", \"Time In Range (%)_total\", \"Time Low (%)_total\", \n",
    "               \"Mean_nighttime\", \"Time Low (%)_nighttime\"]\n",
    "df = all_data[thirty_classes]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "egyptian-croatia",
   "metadata": {},
   "outputs": [],
   "source": [
    "glucose_classes = [\"Mean_total\", \"CV_total\", \"Time High (%)_total\", \"Time In Range (%)_total\", \"Time Low (%)_total\", \n",
    "               \"Mean_nighttime\", \"Time Low (%)_nighttime\"]\n",
    "\n",
    "for class_name in glucose_classes:\n",
    "    df = df[df[class_name] != '.']\n",
    "    df = df[df[class_name] != \"#DIV/0!\"]\n",
    "    labels = labels[labels[class_name] != '.']\n",
    "    labels = labels[labels[class_name] != \"#DIV/0!\"]\n",
    "labels = labels[[\"Participant\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "sitting-transcription",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.',\n",
       " '0',\n",
       " '1',\n",
       " 'N/.A',\n",
       " 'No diaries (baseline)',\n",
       " 'No diaries (exercise not tracked during baseline)',\n",
       " nan}"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify all types of data inputed for binary data\n",
    "set(df[\"Had Negative Feelings_binary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "inclusive-marshall",
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_classes = [\"Planned Day Off_binary\", \"Not Enough Time_binary\",\n",
    "              \"Lacked Positive Feelings_binary\", \"Had Negative Feelings_binary\", \"Lacked Energy_binary\", \n",
    "               \"Felt Fatigued_binary\", \"Blood sugar management issue_binary\", \"Sick\"]\n",
    "\n",
    "for class_name in binary_classes:\n",
    "        df[class_name].replace({np.nan: '0', \"No diaries (baseline)\": '0', \n",
    "                               '.': '0', \"N/.A\": '0', \"No diaries (exercise not tracked during baseline)\": '0'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "celtic-google",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.', '1', '2', '3', '4', '5', 'No diaries (baseline)'}"
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# identify all types of data inputed for 5 scale data\n",
    "set(df[\"Evening Fear of Hypoglycemia\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "established-reunion",
   "metadata": {},
   "outputs": [],
   "source": [
    "five_scale_classes = [\"Morning Fear of Hypoglycemia\",\n",
    "              \"Evening Fear of Hypoglycemia\", \"Sleep Quality\", \"FOHPre\", \"PositivePre\", \"NegativePre\", \n",
    "                \"EnergyPre\", \"FatiguePre\", \"FOHPost\", \"PositivePost\", \"NegativePost\", \"EnergyPost\",\t\"FatiguePost\"]\n",
    "\n",
    "# get column mean\n",
    "def get_col_mean(col_name):\n",
    "    cum_sum = 0\n",
    "    num = 0\n",
    "    for entry in df[col_name]:\n",
    "        try:\n",
    "            if entry.isdigit():\n",
    "                num += 1\n",
    "                cum_sum += float(entry)\n",
    "        except:\n",
    "            pass\n",
    "    return str(int(round(cum_sum/num, 0)))\n",
    "            \n",
    "# replace Nan values with mean\n",
    "for class_name in five_scale_classes:\n",
    "    col_mean = get_col_mean(class_name)\n",
    "    df[class_name].replace({np.nan: col_mean, \"No diaries (baseline)\": col_mean, '.': col_mean}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "athletic-auckland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Exercise_Duration_Total                float64\n",
       "Exercise_METmin_Total                  float64\n",
       "Planned Day Off_binary                 float64\n",
       "Not Enough Time_binary                 float64\n",
       "Lacked Positive Feelings_binary        float64\n",
       "Had Negative Feelings_binary           float64\n",
       "Lacked Energy_binary                   float64\n",
       "Felt Fatigued_binary                   float64\n",
       "Blood sugar management issue_binary    float64\n",
       "Sick                                   float64\n",
       "Morning Fear of Hypoglycemia           float64\n",
       "Evening Fear of Hypoglycemia           float64\n",
       "Sleep Quality                          float64\n",
       "FOHPre                                 float64\n",
       "PositivePre                            float64\n",
       "NegativePre                            float64\n",
       "EnergyPre                              float64\n",
       "FatiguePre                             float64\n",
       "FOHPost                                float64\n",
       "PositivePost                           float64\n",
       "NegativePost                           float64\n",
       "EnergyPost                             float64\n",
       "FatiguePost                            float64\n",
       "Mean_total                             float64\n",
       "CV_total                               float64\n",
       "Time High (%)_total                    float64\n",
       "Time In Range (%)_total                float64\n",
       "Time Low (%)_total                     float64\n",
       "Mean_nighttime                         float64\n",
       "Time Low (%)_nighttime                 float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dict_type = {}\n",
    "for class_name in thirty_classes:\n",
    "    dict_type[class_name] = 'float64'\n",
    "df.astype(dict_type).dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "complicated-preservation",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.to_numpy(dtype='float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "id": "subsequent-delay",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard scaling will convert the data to have a standard normal distrubtion.\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scale_features_std = StandardScaler()\n",
    "features_train = scale_features_std.fit_transform(df)\n",
    "features = scale_features_std.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "id": "comfortable-terrorist",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision                             # contains image datasets and many functions to manipulate images\n",
    "import torchvision.transforms as transforms    # to normalize, scale etc the dataset\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torch.utils.data import DataLoader        # to load data into batches (for SGD)\n",
    "from torchvision.utils import make_grid        # Plotting. Makes a grid of tensors\n",
    "import matplotlib.pyplot as plt                # to plot our images\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "id": "confused-publication",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_x = []\n",
    "my_y = []\n",
    "for a in features:\n",
    "    my_x.append(np.array(a))\n",
    "for b in labels['Participant']:\n",
    "    my_y.append([b])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "id": "material-avenue",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_x = torch.Tensor(my_x) # transform to torch tensor\n",
    "tensor_y = torch.Tensor(my_y) # transform to torch tensor\n",
    "batch_size = 2\n",
    "my_dataset = TensorDataset(tensor_x,tensor_y) # create your datset\n",
    "my_dataloader = DataLoader(my_dataset, batch_size=batch_size, shuffle=True) # create your dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "second-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn                          # Class that implements a model (such as a Neural Network)\n",
    "import torch.nn.functional as F                # contains activation functions, sampling layers etc\n",
    "import torch.optim as optim                    # For optimization routines such as SGD, ADAM, ADAGRAD, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "id": "relative-opposition",
   "metadata": {},
   "outputs": [],
   "source": [
    "e_hidden = 16        # Number of hidden units in the encoder. See AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "d_hidden = 16        # Number of hidden units in the decoder. See AEVB paper page 7, section \"Marginal Likelihood\"\n",
    "latent_dim = 2        # Dimension of latent space. See AEVB paper, page 7, section \"Marginal Likelihood\"\n",
    "learning_rate = 0.001 # For optimizer (SGD or Adam)\n",
    "weight_decay = 1e-5   # For optimizer (SGD or Adam)\n",
    "epochs = 50           # Number of sweeps through the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "id": "dried-creativity",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"Variational Auto-Encoder Class\"\"\"\n",
    "        super(VAE, self).__init__()\n",
    "        \n",
    "        # Encoding Layers\n",
    "        self.e_input2hidden = nn.Linear(in_features=30, out_features=e_hidden)\n",
    "        self.e_hidden2mean = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "        self.e_hidden2logvar = nn.Linear(in_features=e_hidden, out_features=latent_dim)\n",
    "\n",
    "        # Decoding Layers\n",
    "        self.d_latent2hidden = nn.Linear(in_features=latent_dim, out_features=d_hidden)\n",
    "        self.d_hidden2image = nn.Linear(in_features=d_hidden, out_features=30)\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Shape Flatten image to [batch_size, input_features]\n",
    "        x = x.view(-1, 30)\n",
    "        \n",
    "        # Feed x into Encoder to obtain mean and logvar\n",
    "        x = F.relu(self.e_input2hidden(x))\n",
    "        mu, logvar = self.e_hidden2mean(x), self.e_hidden2logvar(x)\n",
    "        \n",
    "        # Sample z from latent space using mu and logvar\n",
    "        if self.training:\n",
    "            z = torch.randn_like(mu).mul(torch.exp(0.5*logvar)).add_(mu)\n",
    "        else:\n",
    "            z = mu\n",
    "        \n",
    "        # Feed z into Decoder to obtain reconstructed image. Use Sigmoid as output activation (=probabilities)\n",
    "        x_recon = torch.sigmoid(self.d_hidden2image(torch.relu(self.d_latent2hidden(z))))\n",
    "        \n",
    "        return x_recon, mu, logvar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "id": "automatic-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss\n",
    "def vae_loss(image, reconstruction, mu, logvar):\n",
    "    # Binary Cross Entropy for batch\n",
    "    BCE = F.binary_cross_entropy(input=reconstruction.view(-1, 30), target=image.view(-1, 30), reduction='sum')\n",
    "    # Closed-form KL Divergence\n",
    "    KLD = 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return BCE - KLD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "id": "level-operation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VAE(\n",
       "  (e_input2hidden): Linear(in_features=30, out_features=16, bias=True)\n",
       "  (e_hidden2mean): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (e_hidden2logvar): Linear(in_features=16, out_features=2, bias=True)\n",
       "  (d_latent2hidden): Linear(in_features=2, out_features=16, bias=True)\n",
       "  (d_hidden2image): Linear(in_features=16, out_features=30, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 502,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate VAE with Adam optimizer\n",
    "vae = VAE()\n",
    "optimizer = optim.Adam(params=vae.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "vae.train()            # tell the network to be in training mode. Useful to activate Dropout layers & other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "id": "comic-jefferson",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1 / 50] average reconstruction error: -39.694299\n",
      "Epoch [2 / 50] average reconstruction error: -252.622801\n",
      "Epoch [3 / 50] average reconstruction error: -355.395301\n",
      "Epoch [4 / 50] average reconstruction error: -421.771513\n",
      "Epoch [5 / 50] average reconstruction error: -471.316629\n",
      "Epoch [6 / 50] average reconstruction error: -511.179231\n",
      "Epoch [7 / 50] average reconstruction error: -548.396109\n",
      "Epoch [8 / 50] average reconstruction error: -587.144404\n",
      "Epoch [9 / 50] average reconstruction error: -619.934631\n",
      "Epoch [10 / 50] average reconstruction error: -653.455774\n",
      "Epoch [11 / 50] average reconstruction error: -678.968237\n",
      "Epoch [12 / 50] average reconstruction error: -705.239102\n",
      "Epoch [13 / 50] average reconstruction error: -734.869038\n",
      "Epoch [14 / 50] average reconstruction error: -757.343582\n",
      "Epoch [15 / 50] average reconstruction error: -780.200760\n",
      "Epoch [16 / 50] average reconstruction error: -804.311627\n",
      "Epoch [17 / 50] average reconstruction error: -815.313085\n",
      "Epoch [18 / 50] average reconstruction error: -831.446719\n",
      "Epoch [19 / 50] average reconstruction error: -854.020825\n",
      "Epoch [20 / 50] average reconstruction error: -863.142386\n",
      "Epoch [21 / 50] average reconstruction error: -883.523963\n",
      "Epoch [22 / 50] average reconstruction error: -904.528144\n",
      "Epoch [23 / 50] average reconstruction error: -914.733437\n",
      "Epoch [24 / 50] average reconstruction error: -940.929116\n",
      "Epoch [25 / 50] average reconstruction error: -949.482237\n",
      "Epoch [26 / 50] average reconstruction error: -964.457664\n",
      "Epoch [27 / 50] average reconstruction error: -976.656719\n",
      "Epoch [28 / 50] average reconstruction error: -992.585467\n",
      "Epoch [29 / 50] average reconstruction error: -1015.781680\n",
      "Epoch [30 / 50] average reconstruction error: -1026.406436\n",
      "Epoch [31 / 50] average reconstruction error: -1036.868776\n",
      "Epoch [32 / 50] average reconstruction error: -1055.854476\n",
      "Epoch [33 / 50] average reconstruction error: -1063.456257\n",
      "Epoch [34 / 50] average reconstruction error: -1068.004197\n",
      "Epoch [35 / 50] average reconstruction error: -1087.277049\n",
      "Epoch [36 / 50] average reconstruction error: -1097.095770\n",
      "Epoch [37 / 50] average reconstruction error: -1106.244180\n",
      "Epoch [38 / 50] average reconstruction error: -1114.521113\n",
      "Epoch [39 / 50] average reconstruction error: -1122.769974\n",
      "Epoch [40 / 50] average reconstruction error: -1129.493831\n",
      "Epoch [41 / 50] average reconstruction error: -1137.857160\n",
      "Epoch [42 / 50] average reconstruction error: -1143.517469\n",
      "Epoch [43 / 50] average reconstruction error: -1151.705616\n",
      "Epoch [44 / 50] average reconstruction error: -1163.594999\n",
      "Epoch [45 / 50] average reconstruction error: -1162.500173\n",
      "Epoch [46 / 50] average reconstruction error: -1170.977960\n",
      "Epoch [47 / 50] average reconstruction error: -1179.310206\n",
      "Epoch [48 / 50] average reconstruction error: -1182.017276\n",
      "Epoch [49 / 50] average reconstruction error: -1196.237583\n",
      "Epoch [50 / 50] average reconstruction error: -1192.969459\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Store training losses & instantiate batch counter\n",
    "    losses.append(0)\n",
    "    number_of_batches = 0\n",
    "\n",
    "    # Grab the batch, we are only interested in images not on their labels\n",
    "    for images, _ in my_dataloader:\n",
    "        # Save batch to GPU, remove existing gradients from previous iterations\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Feed images to VAE. Compute Loss.\n",
    "        reconstructions, latent_mu, latent_logvar = vae(images)\n",
    "        loss = vae_loss(images, reconstructions, latent_mu, latent_logvar)\n",
    "\n",
    "        # Backpropagate the loss & perform optimization step with such gradients\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add loss to the cumulative sum\n",
    "        losses[-1] += loss.item()  \n",
    "        number_of_batches += 1\n",
    "\n",
    "    # Update average loss & Log information\n",
    "    losses[-1] /= number_of_batches\n",
    "    print('Epoch [%d / %d] average reconstruction error: %f' % (epoch+1, epochs, losses[-1]))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enormous-stack",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
